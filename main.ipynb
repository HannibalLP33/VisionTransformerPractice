{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Drills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import mnist\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Log Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = \"logs.log\", format = \"%(asctime)s -- %(message)s\", datefmt='%m/%d/%Y %I:%M:%S %p', level = logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "X_train = X_train[:100]\n",
    "y_train = y_train[:100]\n",
    "logging.info(\"Dataset Upload successfully\")\n",
    "logging.info(f\"X training set shape:{X_train.shape}\")\n",
    "logging.info(f\"y training set shape:{y_train.shape}\")\n",
    "logging.info(f\"X test set shape:{X_test.shape}\")\n",
    "logging.info(f\"y training set shape:{y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "batch_size = 256\n",
    "image_size = 72 \n",
    "patch_size = 6\n",
    "num_patches = int(image_size/patch_size)**2\n",
    "patch_plot_size = int(image_size/patch_size)\n",
    "epochs = 1000\n",
    "projection_dim = 32\n",
    "mlp_dim = 128\n",
    "transformer_layer = 1\n",
    "multiheadattention_heads = 4\n",
    "num_classes = 100\n",
    "transformer_units = [\n",
    "    projection_dim **2,\n",
    "    projection_dim\n",
    "]\n",
    "mlp_units = [2048, 1024]\n",
    "input_shape = (32,32,3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images):\n",
    "    images /=255\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(images):\n",
    "    resized_batch = [tf.image.resize(tf.convert_to_tensor(image), size = (image_size, image_size)) for image in images[1:]]\n",
    "    return tf.convert_to_tensor(resized_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(images, plot_sample = True):\n",
    "    if plot_sample:\n",
    "        #Pick a random image and resize to desired shape\n",
    "        sample_image = images[random.randint(0, len(images))]\n",
    "        sample_image = tf.image.resize(tf.convert_to_tensor(sample_image), size = (image_size, image_size))\n",
    "        logging.info(f\"Sample Image Shape: {sample_image.shape}\")\n",
    "        \n",
    "        # Plotting Original Image\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        fig.add_subplot()\n",
    "        plt.imshow(sample_image)\n",
    "        \n",
    "        #Plotting Patched Image\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        #Output of extract_patches method is a four dimensional tensor with shape (1, number of columns, number of rows, number of elements per patch)\n",
    "        patched_image = tf.image.extract_patches(tf.expand_dims(sample_image,0),\n",
    "                                                sizes = [1, patch_size, patch_size, 1],\n",
    "                                                strides = [1, patch_size,patch_size, 1],\n",
    "                                                rates = [1,1,1,1],\n",
    "                                                padding = \"VALID\") \n",
    "        patched_image = tf.reshape(patched_image, (1,-1,patched_image.shape[-1])) # Output is reshape to be (1 image, number of patches, number of elements per patch)\n",
    "        for i, ax in enumerate(range(patched_image.shape[1])): #For loop to display each patch in the form of the original image\n",
    "            fig.add_subplot(patch_plot_size, patch_plot_size, i + 1)\n",
    "            plt.imshow(tf.reshape(patched_image[0][i], (patch_size,patch_size,3)))\n",
    "            plt.axis(\"off\")\n",
    "    patched_batch = tf.image.extract_patches(images,\n",
    "                                                sizes = [1, patch_size, patch_size, 1],\n",
    "                                                strides = [1, patch_size,patch_size, 1],\n",
    "                                                rates = [1,1,1,1],\n",
    "                                                padding = \"VALID\")\n",
    "    patched_batch = tf.reshape(patched_batch, (patched_batch.shape[0], -1, patched_batch.shape[-1]))\n",
    "    return patched_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(patched_batch):\n",
    "    patch_projection = keras.layers.Dense(projection_dim)\n",
    "    position_embedding = keras.layers.Embedding(input_dim = num_patches, output_dim = projection_dim)\n",
    "    position = tf.range(start=0, limit=num_patches)\n",
    "    encoded = [patch_projection(patched_image)+ position_embedding(position) for patched_image in patched_batch]\n",
    "\n",
    "    return tf.convert_to_tensor(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_block(x, hidden_units, dropout_rate):\n",
    "    for unit in hidden_units:\n",
    "        x = keras.layers.Dense(unit, activation = tf.nn.gelu)(x)\n",
    "        x = keras.layers.Dropout(rate = dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    inputs = keras.Input(shape = input_shape)\n",
    "    resize_norm_inputs = keras.Sequential(\n",
    "        [\n",
    "            layers.Resizing(image_size, image_size),\n",
    "            layers.Rescaling(1./255)\n",
    "        ]\n",
    "    )(inputs[1:])\n",
    "    # norm_inputs = normalize_images(inputs)\n",
    "    # resized_inputs = resize_images(norm_inputs)\n",
    "    patched_inputs = create_patches(resize_norm_inputs, False)\n",
    "    encoded_inputs = positional_encoding(patched_inputs)\n",
    "    \n",
    "    \n",
    "    for _ in range(transformer_layer):\n",
    "        layer1 = keras.layers.LayerNormalization(epsilon = 1e-6)(encoded_inputs)\n",
    "        layer1 = keras.layers.MultiHeadAttention(num_heads = multiheadattention_heads, key_dim = projection_dim, dropout=0.1)(layer1, layer1)\n",
    "        skip1 = keras.layers.Add()([layer1, encoded_inputs])\n",
    "        final_layer = keras.layers.LayerNormalization(epsilon = 1e-6)(skip1)\n",
    "        final_layer = mlp_block(final_layer, transformer_units, dropout_rate = 0.1)\n",
    "        skip2 = keras.layers.Add()([final_layer, skip1])\n",
    "        \n",
    "    encoder_output = keras.layers.LayerNormalization(epsilon = 1e-6)(skip2)\n",
    "    encoder_output = keras.layers.Flatten()(encoder_output)\n",
    "    encoder_output = keras.layers.Dropout(0.5)(encoder_output)\n",
    "    \n",
    "    features = mlp_block(encoder_output, mlp_units, dropout_rate = 0.5)\n",
    "    \n",
    "    logit = keras.layers.Dense(num_classes)(features)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = logit)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate, decay=weight_decay)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss = keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics = keras.metrics.CategoricalAccuracy(name = \"Cat_Accuracy\"))\n",
    "    history = model.fit(\n",
    "        x = X_train,\n",
    "        y = y_train,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"tf.reshape\" (type TFOpLambda).\n\nFailed to convert elements of (None, -1, 108) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received by layer \"tf.reshape\" (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(None, 12, 12, 108), dtype=float32)\n  • shape=('None', '-1', '108')\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb Cell 17\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m transformer \u001b[39m=\u001b[39m encoder()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m history \u001b[39m=\u001b[39m train_model(transformer)\n",
      "\u001b[1;32m/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb Cell 17\u001b[0m in \u001b[0;36mencoder\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m resize_norm_inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         layers\u001b[39m.\u001b[39mResizing(image_size, image_size),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         layers\u001b[39m.\u001b[39mRescaling(\u001b[39m1.\u001b[39m\u001b[39m/\u001b[39m\u001b[39m255\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m )(inputs[\u001b[39m1\u001b[39m:])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# norm_inputs = normalize_images(inputs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# resized_inputs = resize_images(norm_inputs)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m patched_inputs \u001b[39m=\u001b[39m create_patches(resize_norm_inputs, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m encoded_inputs \u001b[39m=\u001b[39m positional_encoding(patched_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(transformer_layer):\n",
      "\u001b[1;32m/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb Cell 17\u001b[0m in \u001b[0;36mcreate_patches\u001b[0;34m(images, plot_sample)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m patched_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mextract_patches(images,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                                             sizes \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, patch_size, patch_size, \u001b[39m1\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                                             strides \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, patch_size,patch_size, \u001b[39m1\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                                             rates \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                                             padding \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mVALID\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m patched_batch \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(patched_batch, (patched_batch\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, patched_batch\u001b[39m.\u001b[39;49mshape[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hannibalpharathikoune/Documents/GitHub/TransformerDrills/main.ipynb#X53sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m patched_batch\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/layers/core/tf_op_layer.py:107\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[0;34m(self, op, args, kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[1;32m    105\u001b[0m     \u001b[39misinstance\u001b[39m(x, keras_tensor\u001b[39m.\u001b[39mKerasTensor)\n\u001b[1;32m    106\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten([args, kwargs])):\n\u001b[0;32m--> 107\u001b[0m   \u001b[39mreturn\u001b[39;00m TFOpLambda(op)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mTypeError\u001b[0m: Exception encountered when calling layer \"tf.reshape\" (type TFOpLambda).\n\nFailed to convert elements of (None, -1, 108) to Tensor. Consider casting elements to a supported type. See https://www.tensorflow.org/api_docs/python/tf/dtypes for supported TF dtypes.\n\nCall arguments received by layer \"tf.reshape\" (type TFOpLambda):\n  • tensor=tf.Tensor(shape=(None, 12, 12, 108), dtype=float32)\n  • shape=('None', '-1', '108')\n  • name=None"
     ]
    }
   ],
   "source": [
    "transformer = encoder()\n",
    "\n",
    "history = train_model(transformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
