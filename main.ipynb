{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Drills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.datasets import mnist\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Log Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = \"logs.log\", format = \"%(asctime)s -- %(message)s\", datefmt='%m/%d/%Y %I:%M:%S %p', level = logging.INFO)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
    "logging.info(\"Dataset Upload successfully\")\n",
    "logging.info(f\"X training set shape:{X_train.shape}\")\n",
    "logging.info(f\"y training set shape:{y_train.shape}\")\n",
    "logging.info(f\"X test set shape:{X_test.shape}\")\n",
    "logging.info(f\"y training set shape:{y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###----------------------------------------------------------------------------------------\n",
    "### Images \n",
    "###----------------------------------------------------------------------------------------\n",
    "y_train = tf.keras.utils.to_categorical(y_train,100)\n",
    "num_classes = 100\n",
    "image_size = 72 \n",
    "patch_size = 6\n",
    "num_patches = int(image_size/patch_size)**2\n",
    "patch_plot_size = int(image_size/patch_size)\n",
    "\n",
    "###----------------------------------------------------------------------------------------\n",
    "### Model\n",
    "###----------------------------------------------------------------------------------------\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-4\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "projection_dim = 32\n",
    "mlp_dim = 128\n",
    "transformer_layer = 10\n",
    "multiheadattention_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim ** 2,\n",
    "    projection_dim\n",
    "]\n",
    "mlp_units = [2048, 1024]\n",
    "input_shape = (num_patches, projection_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing (No Data Augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(images):\n",
    "    images /=255\n",
    "    return images\n",
    "\n",
    "def resize_images(images):\n",
    "    resized_batch = [tf.image.resize(tf.convert_to_tensor(image), size = (image_size, image_size)) for image in images]\n",
    "    return tf.convert_to_tensor(resized_batch)\n",
    "\n",
    "\n",
    "def create_patches(images, plot_sample = True):\n",
    "    ###----------------------------------------------------------------------------------------\n",
    "    ### Patch Creation for sample image\n",
    "    ###----------------------------------------------------------------------------------------\n",
    "    \n",
    "    if plot_sample:\n",
    "        # Pick a random image and resize to desired shape\n",
    "        sample_image = images[random.randint(0, len(images))]\n",
    "        sample_image = tf.image.resize(tf.convert_to_tensor(sample_image), size = (image_size, image_size))\n",
    "        logging.info(f\"Sample Image Shape: {sample_image.shape}\")\n",
    "        \n",
    "        # Plotting Original Image\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        fig.add_subplot()\n",
    "        plt.imshow(sample_image)\n",
    "        \n",
    "        # Plotting Patched Image\n",
    "        fig = plt.figure(figsize=(7,7))\n",
    "        #Output of extract_patches method is a four dimensional tensor with shape (1, number of columns, number of rows, number of elements per patch)\n",
    "        patched_image = tf.image.extract_patches(tf.expand_dims(sample_image,0),\n",
    "                                                sizes = [1, patch_size, patch_size, 1],\n",
    "                                                strides = [1, patch_size,patch_size, 1],\n",
    "                                                rates = [1,1,1,1],\n",
    "                                                padding = \"VALID\") \n",
    "        patched_image = tf.reshape(patched_image, (1,-1,patched_image.shape[-1])) # Output is reshape to be (1 image, number of patches, number of elements per patch)\n",
    "        for i, _ in enumerate(range(patched_image.shape[1])): #For loop to display each patch in the form of the original image\n",
    "            fig.add_subplot(patch_plot_size, patch_plot_size, i + 1)\n",
    "            plt.imshow(tf.reshape(patched_image[0][i], (patch_size,patch_size,3)))\n",
    "            plt.axis(\"off\")\n",
    "            \n",
    "    ###----------------------------------------------------------------------------------------\n",
    "    ### Patch Creation for entire batch\n",
    "    ###----------------------------------------------------------------------------------------       \n",
    "    patched_batch = tf.image.extract_patches(images,\n",
    "                                                sizes = [1, patch_size, patch_size, 1],\n",
    "                                                strides = [1, patch_size,patch_size, 1],\n",
    "                                                rates = [1,1,1,1],\n",
    "                                                padding = \"VALID\")\n",
    "    patched_batch = tf.reshape(patched_batch, (patched_batch.shape[0], -1, patched_batch.shape[-1])) #(Batch Size, number of patches, number of elements per patch)\n",
    "    return patched_batch\n",
    "\n",
    "def positional_encoding(patched_batch):\n",
    "    patch_projection = keras.layers.Dense(projection_dim)\n",
    "    position_embedding = keras.layers.Embedding(input_dim = num_patches, output_dim = projection_dim)\n",
    "    position = tf.range(start=0, limit=num_patches)\n",
    "    encoded = [patch_projection(patched_image) + position_embedding(position) for patched_image in patched_batch]\n",
    "\n",
    "    return tf.convert_to_tensor(encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###----------------------------------------------------------------------------------------\n",
    "### MLP Block for Transformer encoder and MLP Head\n",
    "###----------------------------------------------------------------------------------------\n",
    "def mlp_block(x, hidden_units, dropout_rate):\n",
    "    for unit in hidden_units:\n",
    "        x = keras.layers.Dense(unit, activation = tf.nn.gelu)(x)\n",
    "        x = keras.layers.Dropout(rate = dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder():\n",
    "    inputs = keras.Input(shape = input_shape) #(Batch Size, Number of Patches, Projection Dim)\n",
    "    ###----------------------------------------------------------------------------------------\n",
    "    ### Transformer Architecture \n",
    "    ###----------------------------------------------------------------------------------------\n",
    "    for _ in range(transformer_layer):\n",
    "        layer1 = keras.layers.LayerNormalization(epsilon = 1e-6)(inputs)\n",
    "        layer1 = keras.layers.MultiHeadAttention(num_heads = multiheadattention_heads, key_dim = projection_dim, dropout=0.1)(layer1, layer1)\n",
    "        skip1 = keras.layers.Add()([layer1, inputs])\n",
    "        final_layer = keras.layers.LayerNormalization(epsilon = 1e-6)(skip1)\n",
    "        final_layer = mlp_block(final_layer, transformer_units, dropout_rate = 0.1)\n",
    "        skip2 = keras.layers.Add()([final_layer, skip1])\n",
    "        \n",
    "    encoder_output = tf.reduce_mean(skip2, axis = 1)\n",
    "    features = mlp_block(encoder_output, mlp_units, dropout_rate = 0.5)\n",
    "    \n",
    "    logit = keras.layers.Dense(units = num_classes, kernel_initializer=tf.keras.initializers.zeros)(features)\n",
    "    \n",
    "    model = keras.Model(inputs = inputs, outputs = logit)\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate, decay=weight_decay)\n",
    "    checkpoint_path = \"models/\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor = \"val_Accuracy\", save_best_only = True, save_weights_only = True)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss = keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  metrics = [keras.metrics.CategoricalAccuracy(name = \"Accuracy\")])\n",
    "                  \n",
    "    history = model.fit(\n",
    "        x = encoded_batch,\n",
    "        y = y_train,\n",
    "        batch_size = batch_size,\n",
    "        epochs = epochs,\n",
    "        validation_split = 0.2,\n",
    "        callbacks = [checkpoint_callback],\n",
    "        \n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Clean up needed, rough draft finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = resize_images(X_train)\n",
    "X_train = normalize_images(X_train)\n",
    "patched_batch = create_patches(X_train, True)\n",
    "encoded_batch = positional_encoding(patched_batch) \n",
    "\n",
    "transformer = transformer_encoder()\n",
    "\n",
    "history = train_model(transformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
